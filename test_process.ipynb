{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b01bc5e1a404858b3539e868bc9a6ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4215f79bd9f44456a378f383ef1cde6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a72133d116c4b12881a8af79b419368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/67.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45aaefc118cf4853a14713bfa51c0b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1a52c54dce473597bfff7ec3e222f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/650 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299f837b0bf64a26a339c7090bed8f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49f5f57d43543788316d930d1acf767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe3527217a84ee099ffe22e7dcbfa50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454630bfccd5437ea0405f5def581566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b4d3dba6c548fcaae00c95684ab108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7841bd47deb5443b8f6d0f317bbf1fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"intfloat/e5-base-v2\")\n",
    "\n",
    "sentences = [\n",
    "    \"That is a happy person\",\n",
    "    \"That is a happy dog\",\n",
    "    \"That is a very happy person\",\n",
    "    \"Today is a sunny day\"\n",
    "]\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities.shape)\n",
    "# [4, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9341aadddb4e2991739bd16025194d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8776a59e42349ae8c3c9a6164815f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/72.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8acf669c3a74cad8d9e8fc4091b337a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e08773e2d842a39caf8691e146b947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9c16c489a147ae885b23e33f32cd0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration.py:   0%|          | 0.00/7.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- configuration.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ab4a242b454548986ce3c11ebc19fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling.py:   0%|          | 0.00/59.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- modeling.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5bd8745a2cc4e529cb4e8db6813f4bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/547M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668698fc55b042e9a58cf4fb003bb7a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d9221ec07048469680f253f99bb6d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a190f1d02f9b42629adedbda40013eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a05164fe6ded4a28a944567587560abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6804b68b6b634616aae19de7b8ec5f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# load model\n",
    "# model = SentenceTransformer('intfloat/e5-base-v2')\n",
    "model = SentenceTransformer(\"Alibaba-NLP/gte-base-en-v1.5\", trust_remote_code=True)\n",
    "\n",
    "df = pd.read_csv('paws_samples_1737x6.csv', sep=',', header=None, names=['Index', 'original_sentence', 'modified_sentence', 'relation', 'prediction', 'paraphrase', 'source_file'])\n",
    "df = df.dropna(subset=['original_sentence', 'modified_sentence', 'paraphrase'])\n",
    "\n",
    "df['original_sentence'] = df['original_sentence'].str.strip(' \"')\n",
    "df['modified_sentence'] = df['modified_sentence'].str.strip(' \"')\n",
    "df['paraphrase'] = df['paraphrase'].str.strip(' \"')\n",
    "\n",
    "# similarity computation\n",
    "def compute_similarity(row):\n",
    "    orig = row['original_sentence']\n",
    "    mod = row['modified_sentence']\n",
    "    para = row['paraphrase']\n",
    "    \n",
    "    orig_embedding = model.encode(orig, convert_to_tensor=True)\n",
    "    mod_embedding = model.encode(mod, convert_to_tensor=True)\n",
    "    para_embedding = model.encode(para, convert_to_tensor=True)\n",
    "    \n",
    "    sim_orig_mod = util.cos_sim(orig_embedding, mod_embedding).item()\n",
    "    sim_orig_para = util.cos_sim(orig_embedding, para_embedding).item()\n",
    "    \n",
    "    row['sim_orig_mod'] = sim_orig_mod\n",
    "    row['sim_orig_para'] = sim_orig_para\n",
    "    \n",
    "    return row\n",
    "\n",
    "df = df.apply(compute_similarity, axis=1)\n",
    "\n",
    "df.to_csv('paws_samples_with_similarity_gte.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('intfloat/e5-base-v2')\n",
    "\n",
    "df = pd.read_csv('paws_samples_1737x6.csv', sep=',', header=None, names=['Index', 'original_sentence', 'modified_sentence', 'relation', 'prediction', 'paraphrase', 'source_file'])\n",
    "df = df.dropna(subset=['original_sentence', 'modified_sentence', 'paraphrase'])\n",
    "\n",
    "df['original_sentence'] = df['original_sentence'].str.strip(' \"')\n",
    "df['modified_sentence'] = df['modified_sentence'].str.strip(' \"')\n",
    "df['paraphrase'] = df['paraphrase'].str.strip(' \"')\n",
    "\n",
    "def compute_similarity(row):\n",
    "    orig = row['original_sentence']\n",
    "    mod = row['modified_sentence']\n",
    "    para = row['paraphrase']\n",
    "    \n",
    "    sim_orig_mod = model.similarity([orig], [mod])[0]\n",
    "    sim_orig_para = model.similarity([orig], [para])[0]\n",
    "    \n",
    "    row['sim_orig_mod'] = sim_orig_mod\n",
    "    row['sim_orig_para'] = sim_orig_para\n",
    "    \n",
    "    return row\n",
    "\n",
    "df = df.apply(compute_similarity, axis=1)\n",
    "\n",
    "df.to_csv('paws_samples_with_similarity_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# load model\n",
    "# model = SentenceTransformer('intfloat/e5-base-v2')\n",
    "model = SentenceTransformer(\"Alibaba-NLP/gte-base-en-v1.5\", trust_remote_code=True)\n",
    "\n",
    "df = pd.read_csv('paws_samples_1737x6.csv', sep=',', header=None, names=['Index', 'original_sentence', 'modified_sentence', 'relation', 'prediction', 'paraphrase', 'source_file'])\n",
    "df = df.dropna(subset=['original_sentence', 'modified_sentence', 'paraphrase'])\n",
    "\n",
    "df['original_sentence'] = df['original_sentence'].str.strip(' \"')\n",
    "df['modified_sentence'] = df['modified_sentence'].str.strip(' \"')\n",
    "df['paraphrase'] = df['paraphrase'].str.strip(' \"')\n",
    "\n",
    "# similarity computation\n",
    "def compute_similarity(row):\n",
    "    orig = row['original_sentence']\n",
    "    mod = row['modified_sentence']\n",
    "    para = row['paraphrase']\n",
    "    \n",
    "    orig_embedding = model.encode(orig, convert_to_tensor=True)\n",
    "    mod_embedding = model.encode(mod, convert_to_tensor=True)\n",
    "    para_embedding = model.encode(para, convert_to_tensor=True)\n",
    "    \n",
    "    sim_orig_mod = util.cos_sim(orig_embedding, mod_embedding).item()\n",
    "    sim_orig_para = util.cos_sim(orig_embedding, para_embedding).item()\n",
    "    \n",
    "    # 将相似度添加到DataFrame中\n",
    "    row['sim_orig_mod'] = sim_orig_mod\n",
    "    row['sim_orig_para'] = sim_orig_para\n",
    "    \n",
    "    return row\n",
    "\n",
    "df = df.apply(compute_similarity, axis=1)\n",
    "\n",
    "df.to_csv('paws_samples_with_similarity_gte.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ed758f96ce430dbf7979559eda11ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed03ac9a96240f9a0e3bbe6b2111923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0275176045894d2093b0bc68f7710fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdbfa919bca640b4ab54392e8324af48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e74723c34b384e59af531e9ca8068935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794c27871d7b4c2c9635c53a9780cdbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733696d432a748829e4b74873aaffdf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bacb885a8614341a44b402e424398d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae535d4c9724bb5848d0477faedeed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076fb3539930487286a995dfef48c8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d82575bbc584d798226c84f6a3b7676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "sentences = [\n",
    "    \"That is a happy person\",\n",
    "    \"That is a happy dog\",\n",
    "    \"That is a very happy person\",\n",
    "    \"Today is a sunny day\"\n",
    "]\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities.shape)\n",
    "# [4, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final version of similarity computation\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "df = pd.read_csv('paws_samples_1737x6.csv', sep=',', header=None, names=['original_sentence', 'modified_sentence', 'relation', 'prediction', 'paraphrase', 'source_file'])\n",
    "df = df.dropna(subset=['original_sentence', 'modified_sentence', 'paraphrase'])\n",
    "\n",
    "df['original_sentence'] = df['original_sentence'].str.strip(' \"')\n",
    "df['modified_sentence'] = df['modified_sentence'].str.strip(' \"')\n",
    "df['paraphrase'] = df['paraphrase'].str.strip(' \"')\n",
    "\n",
    "def compute_similarity(row):\n",
    "    orig = row['original_sentence']\n",
    "    mod = row['modified_sentence']\n",
    "    para = row['paraphrase']\n",
    "    \n",
    "    embeddings = model.encode([orig, mod, para])\n",
    "    \n",
    "    similarities = model.similarity(embeddings, embeddings)\n",
    "    \n",
    "    sim_orig_mod = similarities[0, 1].item()  # 原句与修改句的相似度\n",
    "    sim_orig_para = similarities[0, 2].item()  # 原句与释义句的相似度\n",
    "    \n",
    "    row['sim_orig_mod'] = sim_orig_mod\n",
    "    row['sim_orig_para'] = sim_orig_para\n",
    "    \n",
    "    return row\n",
    "\n",
    "df = df.apply(compute_similarity, axis=1)\n",
    "\n",
    "df.to_csv('paws_samples_with_similarity_mpnet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.7640, 0.9666, 0.3148],\n",
      "        [0.7640, 1.0000, 0.7499, 0.3118],\n",
      "        [0.9666, 0.7499, 1.0000, 0.3226],\n",
      "        [0.3148, 0.3118, 0.3226, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on triples (Highest ratio): 0.9131\n",
      "Custom AUC: 0.8963\n"
     ]
    }
   ],
   "source": [
    "#AUC and highest ratio computation\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "df = pd.read_csv('paws_samples_with_similarity_mpnet.csv')\n",
    "\n",
    "def construct_auc_data(df):\n",
    "    \"\"\"\n",
    "    构造 AUC 数据，基于每条样本计算 paraphrase 是否预测正确。\n",
    "    \"\"\"\n",
    "    y_true = []  # ground truth\n",
    "    y_pred = []  # prediction   \n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        sim_orig_mod = row['sim_orig_mod']\n",
    "        sim_orig_para = row['sim_orig_para']\n",
    "        \n",
    "        y_true.extend([0, 1])  #\n",
    "        \n",
    "        y_pred.extend([sim_orig_mod, sim_orig_para])\n",
    "\n",
    "    return y_true, y_pred\n",
    "\n",
    "def calculate_custom_auc(df):\n",
    "    y_true, y_pred = construct_auc_data(df)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    return auc\n",
    "\n",
    "def calculate_highest_ratio(df):\n",
    "    count_highest = 0\n",
    "    for _, row in df.iterrows():\n",
    "        sim_orig_mod = row['sim_orig_mod']\n",
    "        sim_orig_para = row['sim_orig_para']\n",
    "        if sim_orig_para >= max(sim_orig_mod, sim_orig_para):\n",
    "            count_highest += 1\n",
    "    return count_highest / len(df)\n",
    "\n",
    "highest_ratio = calculate_highest_ratio(df)\n",
    "custom_auc = calculate_custom_auc(df)\n",
    "\n",
    "print(f\"Based on triples (Highest ratio): {highest_ratio:.4f}\")\n",
    "print(f\"Custom AUC: {custom_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# List of models to test (can be added or removed as needed)\n",
    "# model_names = [\n",
    "#     \"sentence-transformers/all-mpnet-base-v2\",\n",
    "#     \"Alibaba-NLP/gte-base-en-v1.5\",\n",
    "#     \"intfloat/e5-base-v2\",\n",
    "#     \"sentence-transformers/LaBSE\",\n",
    "#     \"jxm/cde-small-v1\",\n",
    "#     \"avsolatorio/GIST-Embedding-v0\",\n",
    "#     \"nomic-ai/nomic-embed-text-v1.5\",\n",
    "#     \"abhinand/MedEmbed-small-v0.1\",\n",
    "#     \"arkohut/jina-embeddings-v2-base-en\",\n",
    "#     \"Snowflake/snowflake-arctic-embed-m-long\",\n",
    "#     \"hkunlp/instructor-base\",\n",
    "#     \"sentence-transformers/gtr-t5-large\",\n",
    "#     \"Mihaiii/Ivysaur\",\n",
    "#     \"sentence-transformers/sentence-t5-large\",\n",
    "#     \"dunzhang/stella_en_400M_v5\",\n",
    "#     \"sentence-transformers/all-MiniLM-L12-v2\",\n",
    "#     \"nthakur/contriever-base-msmarco\",\n",
    "#     \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "#     \"sentence-transformers/msmarco-bert-co-condensor\",\n",
    "#     \"Muennighoff/SGPT-125M-weightedmean-msmarco-specb-bitfit\",\n",
    "#     \"Mihaiii/Wartortle\",\n",
    "#     \"Mihaiii/Venusaur\",\n",
    "#     \"BAAI/bge-base-en-v1.5\",\n",
    "#     \"consciousAI/cai-stellaris-text-embeddings\",\n",
    "#     \"sentence-transformers/distiluse-base-multilingual-cased-v2\",\n",
    "#     \"sentence-transformers/average_word_embeddings_komninos\",\n",
    "#     \"llmrails/ember-v1\",\n",
    "#     \"qinxianliu/FAB-Ramy-v1\",\n",
    "#     \"sentence-transformers/allenai-specter\",\n",
    "#     # Add or remove models as needed\n",
    "# ]\n",
    "model_names=[\"Mihaiii/gte-micro-v4\"]\n",
    "\n",
    "# Original data file\n",
    "input_csv_path = \"/content/drive/MyDrive/thesis_files/chatgpt_test_samples_11456x6.csv\"\n",
    "\n",
    "# New directory to save similarity results for each model\n",
    "similarity_output_dir = \"/content/drive/MyDrive/thesis_files/neutral_results\"\n",
    "os.makedirs(similarity_output_dir, exist_ok=True)\n",
    "\n",
    "# New directory to save triple accuracy results for each model\n",
    "accuracy_output_dir = \"/content/drive/MyDrive/thesis_files/triple_accuracy_results\"\n",
    "os.makedirs(accuracy_output_dir, exist_ok=True)\n",
    "\n",
    "# Read and preprocess input data\n",
    "df_original = pd.read_csv(\n",
    "    input_csv_path,\n",
    "    sep=',',\n",
    "    header=None,\n",
    "    names=['original_sentence', 'modified_sentence', 'modification_type',\n",
    "           'semantic_relationship', 'paraphrase', 'source_file']\n",
    ")\n",
    "\n",
    "# Remove invalid rows\n",
    "df_original = df_original.dropna(subset=['original_sentence', 'modified_sentence', 'paraphrase'])\n",
    "\n",
    "# Data cleaning\n",
    "df_original['original_sentence'] = df_original['original_sentence'].astype(str).str.strip(' \"')\n",
    "df_original['modified_sentence'] = df_original['modified_sentence'].astype(str).str.strip(' \"')\n",
    "df_original['paraphrase']       = df_original['paraphrase'].astype(str).str.strip(' \"')\n",
    "\n",
    "def compute_similarity_for_df(df, model):\n",
    "    \"\"\"\n",
    "    Given a model and DataFrame, compute sim_orig_mod and sim_orig_para and return the new DataFrame.\n",
    "    \"\"\"\n",
    "    def compute_similarity(row):\n",
    "        orig = row['original_sentence']\n",
    "        mod = row['modified_sentence']\n",
    "        para = row['paraphrase']\n",
    "\n",
    "        embeddings = model.encode([orig, mod, para])\n",
    "        similarities = model.similarity(embeddings, embeddings)\n",
    "\n",
    "        row['sim_orig_mod']  = similarities[0, 1].item()  # Similarity between original and modified sentences\n",
    "        row['sim_orig_para'] = similarities[0, 2].item()  # Similarity between original and paraphrase sentences\n",
    "        return row\n",
    "\n",
    "    df = df.apply(compute_similarity, axis=1)\n",
    "    return df\n",
    "\n",
    "def compute_triple_accuracy(df_group):\n",
    "    \"\"\"\n",
    "    Compute the triple accuracy for a group (under the same modification_type).\n",
    "    Triple accuracy refers to cases where sim_orig_para >= sim_orig_mod being counted as correct.\n",
    "    \"\"\"\n",
    "    count_highest = 0\n",
    "    for _, row in df_group.iterrows():\n",
    "        sim_orig_mod  = row['sim_orig_mod']\n",
    "        sim_orig_para = row['sim_orig_para']\n",
    "        if sim_orig_para >= sim_orig_mod:\n",
    "            count_highest += 1\n",
    "    return count_highest / len(df_group)\n",
    "\n",
    "# To record model loading/prediction status\n",
    "model_status = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"=== Processing model: {model_name} ===\")\n",
    "\n",
    "    model_load_success = True\n",
    "    error_message = \"\"\n",
    "\n",
    "    # Attempt to load the model\n",
    "    try:\n",
    "        model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "    except Exception as e:\n",
    "        # If the model doesn't support trust_remote_code, try without it\n",
    "        try:\n",
    "            print(f\"【警告】Model {model_name} failed to load with trust_remote_code=True, trying without. Error: {str(e)}\")\n",
    "            model = SentenceTransformer(model_name)\n",
    "        except Exception as e2:\n",
    "            # If both attempts fail, skip the model\n",
    "            print(f\"【错误】Model {model_name} failed both loading attempts. Error: {str(e2)}\")\n",
    "            model_load_success = False\n",
    "            error_message = f\"{str(e)} | {str(e2)}\"\n",
    "\n",
    "    if not model_load_success:\n",
    "        # Record failure status and skip further processing\n",
    "        model_status.append({\n",
    "            \"model_name\": model_name,\n",
    "            \"status\": \"failed\",\n",
    "            \"error_message\": error_message\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # If loaded successfully, proceed with computation\n",
    "    df = df_original.copy()\n",
    "    df = compute_similarity_for_df(df, model)\n",
    "\n",
    "    # Write results to new CSV (similarity results)\n",
    "    safe_model_name = model_name.replace(\"/\", \"_\").replace(\":\", \"_\")\n",
    "    similarity_csv_path = os.path.join(similarity_output_dir, f\"{safe_model_name}_predictions.csv\")\n",
    "    df.to_csv(similarity_csv_path, index=False)\n",
    "\n",
    "    # Compute triple accuracy grouped by modification_type\n",
    "    grouped_accuracy = df.groupby('modification_type').apply(compute_triple_accuracy)\n",
    "\n",
    "    # Convert to DataFrame for CSV writing\n",
    "    result_df = pd.DataFrame({\n",
    "        'modification_type': grouped_accuracy.index,\n",
    "        'triple_accuracy': grouped_accuracy.values\n",
    "    })\n",
    "    # Record current model name\n",
    "    result_df.insert(0, 'model_name', model_name)\n",
    "\n",
    "    # Write to a new CSV file (triple accuracy results)\n",
    "    accuracy_csv_path = os.path.join(accuracy_output_dir, f\"{safe_model_name}_triple_accuracy.csv\")\n",
    "    result_df.to_csv(accuracy_csv_path, index=False)\n",
    "\n",
    "    print(f\"Model {model_name} processed. Results saved to:\")\n",
    "    print(f\"  - Similarity results: {similarity_csv_path}\")\n",
    "    print(f\"  - Triple accuracy results: {accuracy_csv_path}\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "\n",
    "    # Record success status\n",
    "    model_status.append({\n",
    "        \"model_name\": model_name,\n",
    "        \"status\": \"success\",\n",
    "        \"error_message\": \"\"\n",
    "    })\n",
    "\n",
    "print(\"All models processed.\")\n",
    "\n",
    "# Summarize and output loading status (can be saved to CSV or printed directly)\n",
    "status_df = pd.DataFrame(model_status)\n",
    "print(\"Model loading/prediction status summary:\")\n",
    "print(status_df)\n",
    "\n",
    "# If needed, write the loading/prediction status to a file here:\n",
    "# status_df.to_csv(\"model_load_status.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
